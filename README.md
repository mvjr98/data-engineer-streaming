# ğŸš€ Real-Time CDC Pipeline: PostgreSQL to Snowflake

Este projeto implementa um pipeline de dados **Change Data Capture (CDC)** de alta performance. O objetivo Ã© replicar transaÃ§Ãµes de um banco operacional (OLTP) para um Data Warehouse (OLAP) com latÃªncia de segundos, utilizando **Snowpipe Streaming** e uma arquitetura robusta de ingestÃ£o e consolidaÃ§Ã£o.

## Tecnologias Utilizadas

<div align="center">

<table>
  <tr>
    <td align="center">
      <a href="https://www.docker.com/" target="_blank">
        <img src="https://raw.githubusercontent.com/mvjr98/fancy-icons/main/docker/docker.svg" width="48" alt="Docker"/>
      </a>
    </td>
    <td align="center">
      <a href="https://www.postgresql.org/" target="_blank">
        <img src="https://raw.githubusercontent.com/mvjr98/fancy-icons/main/postgres/postgres.svg" width="48" alt="PostgreSQL"/>
      </a>
    </td>
    <td align="center">
      <a href="https://kafka.apache.org/" target="_blank">
        <img src="https://raw.githubusercontent.com/mvjr98/fancy-icons/main/apache_kafka/apache_kafka.svg" width="48" alt="Kafka"/>
      </a>
    </td>
    <td align="center">
      <a href="https://www.snowflake.com/pt_br/" target="_blank">
        <img src="https://raw.githubusercontent.com/MvJr98/fancy-icons/main/snowflake/snowflake.svg" width="48" alt="Snowflake"/>
      </a>
    </td>
  </tr>
</table>

</div>


## ğŸ›ï¸ Arquitetura

O fluxo de dados segue a arquitetura abaixo:

1.  **Origem (PostgreSQL):** As transaÃ§Ãµes ocorrem no banco `northwind`.
2.  **Captura (Debezium):** O conector lÃª o *Write-Ahead Log (WAL)* do Postgres.
3.  **Transporte (Kafka):** Os dados sÃ£o serializados em JSON e enviados para tÃ³picos no Kafka Broker.
4.  **IngestÃ£o (Snowpipe Streaming):** O conector Snowflake Sink lÃª do Kafka e faz a ingestÃ£o via gRPC diretamente para tabelas no Snowflake.
5.  **TransformaÃ§Ã£o (Snowflake Tasks):** Uma *Task* agendada faz o `MERGE` (DeduplicaÃ§Ã£o, Updates e Deletes) da tabela de ingestÃ£o (Raw) para a tabela final (Bronze).

![Architecture Diagram](./architecture_diagram.png)

## âš–ï¸ DecisÃ£o de Arquitetura (ADR)

Para a camada de transformaÃ§Ã£o no Snowflake (CDC Merge), existem duas abordagens modernas. Este projeto adota intencionalmente a abordagem **Imperativa (Stream + Task)**.

### ğŸ”¹ Escolha Atual: Stream + Task (Imperativo)
Optou-se por controlar manualmente o ciclo de vida dos dados.
* **Controle Total:** Permite implementar lÃ³gicas complexas de `MERGE` (ex: tratamento de *Deletes* lÃ³gicos via SMT rewrite).
* **Aprendizado:** Excelente para entender a mecÃ¢nica de deduplicaÃ§Ã£o e ordenaÃ§Ã£o de eventos em sistemas distribuÃ­dos.
* **Custo & Performance:** EficiÃªncia ajustada pelo agendamento da Task (CRON) e tamanho do Warehouse, evitando processamento desnecessÃ¡rio.

### ğŸ”¸ Alternativa: Dynamic Tables (Declarativo)
Reconhecemos que *Dynamic Tables* sÃ£o uma alternativa viÃ¡vel.
* **Abordagem:** Define-se apenas o `SELECT` final e o `TARGET_LAG`. O Snowflake gerencia a orquestraÃ§Ã£o.
* **Trade-off:** Ganha-se facilidade de manutenÃ§Ã£o ("Set and Forget"), mas perde-se a granularidade de controle sobre como cada linha Ã© processada e tratada em cenÃ¡rios de borda.

---

## ğŸ“‚ Estrutura do Projeto

```bash
â”œâ”€â”€ Postgres/               # Ambiente do Banco de Origem
â”‚   â”œâ”€â”€ docker-compose.yml  # Postgres + pgAdmin
â”‚   â””â”€â”€ initdb/             # Scripts de DDL e DML (Northwind)
â”‚
â”œâ”€â”€ kafka/                  # Core de Streaming
â”‚   â”œâ”€â”€ docker-compose.yml  # Zookeeper, Broker, Schema Registry, Connect, AKHQ
â”‚   â””â”€â”€ connectors-config/  # JSONs de configuraÃ§Ã£o dos conectores
â”‚
â”œâ”€â”€ Snowflake/              # Scripts e ConfiguraÃ§Ãµes do Destino
â”‚   â””â”€â”€ setup_pipeline.md  # SQL para criar DB, Schema, Tables, Streams e Tasks
â”‚
â”œâ”€â”€ setup_connectors.sh     # Script para automatizar o deploy dos conectores
â””â”€â”€ README.md               # DocumentaÃ§Ã£o do Projeto
```
##
### ğŸ› ï¸ PrÃ©-requisitos
- Docker e Docker Compose instalados.

- [Conta no Snowflake](https://signup.snowflake.com/?trial=student) (Trial ou Enterprise).

- Chaves RSA geradas para autenticaÃ§Ã£o segura no Snowflake.

##

### Como Executar
### 1. Preparar o Ambiente Snowflake
Execute o script SQL localizado em Snowflake/setup_pipeline.md na sua conta Snowflake para criar:

    - UsuÃ¡rio de serviÃ§o (SNFLK_USER_KAFKA) e Roles.

    - Databases (RAW_KAFKA, BRONZE).

    - Tabelas (ORDERS_INGEST, ORDERS).

    - Importante: Configure a Chave PÃºblica RSA no usuÃ¡rio criado.

### 2. Iniciar o Banco de Dados (Origem)
Suba o banco de dados e popule com os dados iniciais:

```bash
cd Postgres
docker-compose up -d
```
ValidaÃ§Ã£o: Acesse o pgAdmin em http://localhost:5050.

### 3. Iniciar o Cluster Kafka
Suba os serviÃ§os de mensageria e o Kafka Connect:

```bash
cd ../kafka
docker-compose up -d
```
ValidaÃ§Ã£o: Acesse o AKHQ (Kafka UI) em http://localhost:8080 para monitorar os tÃ³picos e conectores.

### 4. Deploy dos Conectores (AutomaÃ§Ã£o)
Para configurar os conectores automaticamente, utilize o script ```setup_connectors.sh``` na raiz do projeto.

```bash
chmod +x setup_connectors.sh
./setup_connectors.sh
```
##
### âš™ï¸ Detalhes de ConfiguraÃ§Ã£o
#### Source Connector (Debezium PostgreSQL)
    - Plugin: pgoutput (decodificaÃ§Ã£o lÃ³gica nativa do Postgres 10+).

    - Snapshot Mode: initial (realiza carga histÃ³rica inicial e depois muda para streaming).

    - Topic Prefix: cdc (ex: cdc.public.orders).

    - Tombstones: Desativados (tombstones.on.delete=false), pois o tratamento de delete Ã© feito no Sink via SMT.

#### Sink Connector (Snowflake Streaming)
    - Ingestion Method: SNOWPIPE_STREAMING (Alta performance e baixa latÃªncia via gRPC).

    - Buffer Flush: 1 segundo (Configurado para Near Real-Time).

    - SMT (Single Message Transform): Utiliza ExtractNewRecordState para "aplanar" a estrutura complexa do Debezium e extrair metadados essenciais (__op, __source_ts_ms) para o controle de versÃ£o no Snowflake.
##

ğŸ›¡ï¸ SeguranÃ§a
Este projeto utiliza Key Pair Authentication (RSA 2048) para comunicaÃ§Ã£o entre o Kafka Connect e o Snowflake.

Nunca commite o arquivo da chave privada (rsa_key.p8) no Git.

Utilize o .gitignore para excluir arquivos de chaves e configuraÃ§Ãµes sensÃ­veis.

Em produÃ§Ã£o, recomenda-se o uso de Secrets Management ou variÃ¡veis de ambiente para injetar a chave privada (SNOWFLAKE_PRIVATE_KEY) no container.